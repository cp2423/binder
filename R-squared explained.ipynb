{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4cc579",
   "metadata": {},
   "source": [
    "# R-squared explained\n",
    "\n",
    "Here is one definition of R-squared from [Statistics By Jim](https://statisticsbyjim.com/regression/interpret-r-squared-regression/):\n",
    "\n",
    "> R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively. R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 â€“ 100% scale.\n",
    "\n",
    "Definition from [scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score):\n",
    ">The coefficient $ R^2 $ is defined as $ (1 - \\frac{u}{v}) $, where $ u $ is the residual sum of squares ``((y_true - y_pred)** 2).sum()`` and $ v $ is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a $ R^2 $ score of 0.0.\n",
    "\n",
    "You may recall the same thing written in a slightly different way:\n",
    "> $$ R^2 = 1 - \\frac{RSS}{TSS} $$\n",
    "\n",
    "#### Let's break this down into the following parts:\n",
    "1. Introduce some data and make a linear regression model\n",
    "1. Understand residual sum of squares (RSS)\n",
    "1. Understand total sum of squares (TSS)\n",
    "1. Finally just do: `1 - (RSS/TSS)`\n",
    "\n",
    "Afterwards we will also look into the contradiction in the definitions above around whether $ R^2 $ can be negative or not, plus how to interpret the value i.e. \"what is a *good* $ R^2 $ score?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3424285",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1 of 4 - Introduce some data and make a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data where each y value is x +/- 1\n",
    "df = pd.DataFrame({'x':[1,2,3,4,1,2,3,4], 'y':[2,1,4,3,0,3,2,5]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3fbac",
   "metadata": {},
   "source": [
    "### Visualise\n",
    "\n",
    "`sns.regplot()` visualises the line of best fit through our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fbaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there seems to be a convention to name the plot g\n",
    "# perhaps for 'graph' ??\n",
    "g = sns.regplot(data=df, x='x', y='y')\n",
    "g.set(xlim=(0,6), ylim=(0,6));  # fix x and y axes to start at 0 and end at 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9012f44e",
   "metadata": {},
   "source": [
    "### Build the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a40bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dfe22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['x']]\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b99f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X,y)  # note the () after LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d328c",
   "metadata": {},
   "source": [
    "What `.fit()` does:\n",
    "\n",
    "1. Tries lines in different positions until it finds the line with the least possible error between the line and every data point aka \"the line of best fit\"\n",
    "1. The equation of any straight line is $ y = mx + c $ where $ m $ is the coefficient (gradient) of the line and $ c$ is the intercept (where the line crosses the y-axis). Once `.fit()` has found the line of best fit it then calculates $ m $ and $ c $\n",
    "\n",
    "Note that if we include multiple features in `X` e.g. `X = df[['feature1', 'feature2', 'feature3']]` then this will give multiple coefficients $ m $ so `.fit()` actually caluclates a **list** of coefficients i.e. [$ m_0, m_1, m_2, \\ldots ,m_{x-1} $] where $ x $ is the number of features used, even if there is only one i.e. [$ m_0 $]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = lr.coef_[0]  # can have multiple coefficients so this is a list, use [0] to get only the first value\n",
    "c = lr.intercept_\n",
    "print(f'Equation for line of best fit from LR model: y = {m}x + {c}')  # note f before '' makes {m} and {c} work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d61d4",
   "metadata": {},
   "source": [
    "*NB - the data were chosen deliberately so that m=1 and c=0 to keep things simple. Both the coefficient and the intercept can take any value (including negative values) and in the real world are rarely as neat and tidy as in this artificial example.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eddb1b",
   "metadata": {},
   "source": [
    "## Part 2 of 4 - Understand residual sum of squares (RSS)\n",
    "\n",
    "For each *x* value in `df['x']` we know the **true** value of *y* which we have in `df['y']`.\n",
    "\n",
    "We can now use our newly fitted linear regression module to **predict** the value of *y* for each value of *x*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0cda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model i.e. put each x value into equation y = 1x + 0\n",
    "y_pred = lr.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e775ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store these values in our dataframe to see better what we have got\n",
    "df['y_pred'] = y_pred\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a1a5c",
   "metadata": {},
   "source": [
    "Unless our model is perfect, there will be a **difference** between the true value of each *y* and our predicted *y*. The difference between two values is also called the *residual*.\n",
    "\n",
    "We have also talked about the difference between true *y* and predicted *y* using the word *error* which is also appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ecd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try and visualise this\n",
    "# adapted from https://stackoverflow.com/questions/44941082/plot-multiple-columns-of-pandas-dataframe-using-seaborn\n",
    "df_melted = df.melt('x', var_name='label', value_name='value')\n",
    "g = sns.scatterplot(data=df_melted, x='x', y='value', hue='label')\n",
    "g.set(xlim=(0,6), ylim=(0,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5579243",
   "metadata": {},
   "source": [
    "*NB - there are 8 predictions, one for each value of x, but we see only 4 orange dots because some of the prediction values are the same, therefore we have dots on top of each other so only 4 are visible.*\n",
    "\n",
    "*One solution would be to chop the data into two halves:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80759203",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(13,4))\n",
    "\n",
    "df_melted_0 = df[:4].melt('x', var_name='label', value_name='value')\n",
    "sns.scatterplot(data=df_melted_0, x='x', y='value', hue='label', ax=ax[0])\n",
    "ax[0].set(xlim=(0,6), ylim=(0,6))\n",
    "\n",
    "df_melted_1 = df[4:].melt('x', var_name='label', value_name='value')\n",
    "sns.scatterplot(data=df_melted_1, x='x', y='value', hue='label', ax=ax[1])\n",
    "ax[1].set(xlim=(0,6), ylim=(0,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa42e9d",
   "metadata": {},
   "source": [
    "Two takeaways:\n",
    "1. the predictions all appear on the line calculated by `.fit(X,y)` as this is how linear regression modelling works! Remember our data was chosen deliberately so that m=1 and c=0 to keep things simple in this example.\n",
    "1. the difference (aka the *residual*, aka the *error*) between the true value of `y` (blue dot) and `y_pred` (orange dot) is either +1 or -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76af3d0",
   "metadata": {},
   "source": [
    "### What does this have to do with the \"residual sum of squares (RSS)\" ?\n",
    "\n",
    "In the scikit learn definition at the top of this page we had:\n",
    "\n",
    "> $ R^2 $ is defined as $ (1 - \\frac{u}{v}) $, where $ u $ is the residual sum of squares ``((y_true - y_pred)** 2).sum()``\n",
    "\n",
    "Now we have both `y_true` (in `df['y']`) and also `y_pred` (in `df['y_pred']`) we can calculate $ u $ aka $ RSS $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the difference (residual) between y_true and y_pred for each value of x\n",
    "residuals = df['y'] - df['y_pred']\n",
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square each residual\n",
    "# NB in python we use **2 to square a number, in other languages this would be ^2\n",
    "residual_squares = residuals**2\n",
    "residual_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4af622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the squared residuals to give the \"residual sum of squares\"\n",
    "rss = residual_squares.sum()\n",
    "rss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a29120",
   "metadata": {},
   "source": [
    "So in our example the $ RSS = 8 $\n",
    "\n",
    "#### Aside: RSS and MSE\n",
    "We mentioned that these 'residuals' could also be described as 'errors' since they are the difference between each true value of *y* and the value of *y* predicted by our model.\n",
    "\n",
    "In fact, $ RSS $ is very closely related to the *mean squared error* metric ($ MSE $) that we used in the linear regression bootcamp. Except that instead of the word *residual* we are using *error* and instead of  calculating the *residual **sum** of squares* we calculate the ***mean*** *squared error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d03e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show that the *mean* of our residual squares is 1.0\n",
    "residual_squares.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the MSE metric like we did in the bootcamp\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show that the mean squared error is also 1.0\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb42098",
   "metadata": {},
   "source": [
    "## Part 3 - Understand total sum of squares (TSS)\n",
    "\n",
    "As you might guess from the similar name $ TSS $ is calculated in a similar way to $ RSS $. The difference being that in $ TSS $ we are interested in the difference between each *y* value and the **mean of all our *y* values**, whereas in $ RSS $ above we calculated the difference between each *y* and the **predicted value of *y* given by our model**.\n",
    "\n",
    "We can see this similarity further by putting the relevant parts of the scikit learn equation side by side:\n",
    "\n",
    "$ u $ aka $ RSS $ ``((y_true - y_pred)** 2).sum()``\n",
    "\n",
    "$ v $ aka $ TSS $ ``((y_true - y_true.mean()) ** 2).sum()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c769fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of y\n",
    "y_mean = df['y'].mean()\n",
    "y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c41ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store in our dataframe so that we can plot it\n",
    "df['y_mean'] = y_mean\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-use code from earlier but change y to y='y_mean'\n",
    "g = sns.regplot(data=df, x='x', y='y_mean')\n",
    "g.set(xlim=(0,6), ylim=(0,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5929c94",
   "metadata": {},
   "source": [
    "In a similar way to before, let's look at the difference between each true *y* value and `y_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77317ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise this with our data\n",
    "# NB use of .drop() to exclude the y_pred column from this visualisation\n",
    "df_melted = df.drop(columns=['y_pred']).melt('x', var_name='label', value_name='value')\n",
    "g = sns.scatterplot(data=df_melted, x='x', y='value', hue='label')\n",
    "g.set(xlim=(0,6), ylim=(0,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa1d99",
   "metadata": {},
   "source": [
    "*NB - again, there are 8 orange dots here, just some are on top of each other*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously we looked at the difference (residuals/error) between y and y_pred for each value of x\n",
    "# here we do similar but looking at the difference between y and y_mean\n",
    "diff = df['y'] - df['y_mean']\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ccb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square them\n",
    "diff_squares = diff**2\n",
    "diff_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the squares to give the \"total sum of squares\"\n",
    "tss = diff_squares.sum()\n",
    "tss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8b59b",
   "metadata": {},
   "source": [
    "We previously calculated that our $ RSS = 8 $ and now we have $ TSS = 18 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b8a81",
   "metadata": {},
   "source": [
    "## Part 4 of 4 - Finally just do $ 1 - (RSS/TSS) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6951105",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (rss/tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does this match the R-squared score given by scikit learn?\n",
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42431b7b",
   "metadata": {},
   "source": [
    "#### How to use `.score()`\n",
    "\n",
    "The `.score()` function can cause confusion around what values should be used - for example should the *y* values be `y` or `y_pred` or even something else entirely?\n",
    "\n",
    "Here is a simplified version of the [scikit learn documentation for `.score()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score):\n",
    "\n",
    ">**Parameters:**\n",
    ">\n",
    ">**X** - test samples\n",
    ">\n",
    ">**y** - true values of `X`\n",
    ">\n",
    ">**Returns:**\n",
    ">\n",
    ">**score** - $ R^2 $ of `self.predict(X)` wrt `y` \n",
    "\n",
    "This tells us that the first parameter needs to be our *X* values. As noted in the final line, these *X* values will be used to do `y_pred = predict(X)` \"under the hood\" and then in turn these `y_pred` values will be compared to the second parameter, the true *y* values for each *X*, to calculate the $ R^2 $ score.\n",
    "\n",
    "Takeaway:\n",
    "\n",
    "This is correct `lr.score(X, y)`\n",
    "\n",
    "This would be wrong `lr.score(X, y_pred)` because although it might *feel* like the right thing to do, this would take our predicted *y* values and compare them with the predicted *y* values calculated by `.score()` which would always give a score of 1 since we are comparing values with themselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrating what happens if we do lr.score(X, y_pred) by mistake\n",
    "lr.score(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c22a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interpreting the score\n",
    "\n",
    "Having seen how to calculate $ R^2 $ it is important to understand how to interpret this score and to be clear on whether the score can ever be a negative number or not.\n",
    "\n",
    "### What the score actually tells us\n",
    "\n",
    "Let's return to the definition of $ R^2 $ from Statistics By Jim earlier, which includes the line:\n",
    "\n",
    "> This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.\n",
    "\n",
    "The *dependent variable* is the feature we put on the y-axis and is the thing that we are investigating/modeling, for example this might be the number of bike rentals on a given day.\n",
    "\n",
    "The *independent variables* are the features (one or more) we put on the x-axis and are the things that we think might influence the feature that we are investigating/modeling, for example daily temperature or if a given day is a workday in the bike rentals scenario.\n",
    "\n",
    "In our example above we got a score of 0.5555555555555556 which means that we can say **\"55.56% of the variance in *y* can be explained by *X*\"**. Conversely, this implies that there is 44.44% which is explained by other features that we have not included in our model.\n",
    "\n",
    "The word *variance* is key and has a set definition in statistics. *Variance* [can be defined as follows](https://www.scribbr.com/statistics/variance/):\n",
    "> The variance is a measure of variability. It is calculated by taking the average of squared deviations from the mean.\n",
    "\n",
    "In other words, some of *y* values will be quite close to the mean value of *y*, some values will be further away. Our $ R^2 $ score tells us *how much* (what percentage) of this variation is explained by our model.\n",
    "\n",
    "#### Aside: $ RSS $ and $ TSS $ are closely related to mean squared error and variance\n",
    "\n",
    "Both $ RSS $ and *mean squared error* use the squared deviations from the predicted values from our model. Again, the only difference is that $ RSS $ uses sum whereas *mean squared error* uses the average.\n",
    "\n",
    "Both $ TSS $ and *variance* use the squared deviations from the mean of *y*, the only difference is that $ TSS $ uses the sum of these whereas *variance* uses the average.\n",
    "\n",
    "### What is a \"good\" score?\n",
    "\n",
    "This almost entirely **depends on the context** in which you are working. Let's say your model has $ R^2 = 0.3 $ so less than a third of the variation in your *y* variable is explained by your model. You might think that this is not very good. In most real-world scenarios, however, the are a **huge** number of factors that could have an impact on the thing that you are trying to model.\n",
    "\n",
    "Take bike rentals. How many crazy factors could you dream up that could feasibly affect the number of people that decide to hire a bicycle on any given day? Here are some ideas:\n",
    "\n",
    "* what condition are the local cycle routes in?\n",
    "* is there an event on which might drive hires up? (Or down?)\n",
    "* is there a social media craze going on for kids filming themselves on hire bikes?\n",
    "* how is the economy doing, do people have spare money for bike rides?\n",
    "\n",
    "There could be **literally millions** of potential factors, so having 30% of the variation explained by your model is good.\n",
    "\n",
    "If instead you are doing some sort of highly controlled, scientific study and there is literally only one or two other factors that could potentially affect your *y* values, then having only 30% explained by your model is rubbish - try modelling those other factors instead!\n",
    "\n",
    "It also depends on exactly **what are you trying to achieve** with your model. Knowing that a full 30% of the variation in bike hires is explained by only one or two factors could be a hugely valuable insight for this business. If instead they are trying to create a really robust model which accurately predict bike hires in the future then 0.3 is unlikely to cut it - but even then the business should remember that [predicting the future is hard](http://www.rinkworks.com/said/predictions.shtml) bordering on impossible.\n",
    "\n",
    "Lastly, it is worth noting that a partiucularly high score for a predictive model is not a good idea as this would suggest that the model has been overfitted.\n",
    "\n",
    "It is worth reading the whole [Statistics By Jim piece on R-squared](https://statisticsbyjim.com/regression/interpret-r-squared-regression/) for how to interpret scores but bear in mind that it does include an error when it says that R-squared is always between 0 and 100%, because R-squared can be negative.\n",
    "\n",
    "### How can a square number ever be negative?\n",
    "\n",
    "Basic maths says that any number which has been squared (multiplied by itself) cannot have a negative value because a negative number multiplied by another negative number (in this case, itself) gives a positive number!\n",
    "\n",
    "There are many sources on the internet which claim that R-squared will always be $ 0 <= R^2 <= 1 $ such as the Statistics By Jim piece above.\n",
    "\n",
    "Where does this confusion come from? Let's start by understanding R.\n",
    "\n",
    "#### What is $ R $\n",
    "\n",
    "$ R $ is a common shorthand for [Pearson's correlation coefficient](https://en.wikipedia.org/wiki/Correlation_coefficient) which is the basic measure of correlation that we are familar with, where +1 is a perfect positive correlation, -1 is a perfect negative correlation, and everything in between.\n",
    "\n",
    "We have used the `.corr()` function before, often to make a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a heatmap of these correlations\n",
    "sns.heatmap(data=df.corr(), annot=True, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771e3a8",
   "metadata": {},
   "source": [
    "#### R-squared *is* indeed $ R^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab only the correlation between x and y from the correlation table\n",
    "R = df.corr()['x']['y']\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and square it\n",
    "R**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this matches\n",
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5401c5",
   "metadata": {},
   "source": [
    "We have shown that R-squared is indeed a number ($ R $ the correlation between *x* and *y* in our model) multiplied by itself, therefore it *has to be positive*. This helps explain why lots of sources claim that R-squared must always be between 0 and 1.\n",
    "\n",
    "Which begs the question: so how can R-squared ever be negative? Are we about to re-write the rules of maths?!\n",
    "\n",
    "The first thing to understand is there is a boring, practical reason why R-squared should never be negative, which is that any model with $ R^2 < 0 $ *should not be used* since a negative R-squared value tells us that the model is less good than simply using the mean of *y* to predict all our *y* values.\n",
    "\n",
    "This is because, by definition, a model which used `y_mean` to predict all the *y* values would have an R-squared score of 0. To understand why, look again at the relevant parts of the scikit learn equation side by side:\n",
    "\n",
    ">$ u $ aka $ RSS $ ``((y_true - y_pred) ** 2).sum()``\n",
    ">\n",
    ">$ v $ aka $ TSS $ ``((y_true - y_true.mean()) ** 2).sum()``\n",
    "\n",
    "If we use the mean of *y* (which scikit learn refers to as `y_true.mean()`) in place of `y_pred` in the RSS formula then we get:\n",
    "\n",
    ">$ u $ aka $ RSS $ ``((y_true - y_true.mean()) ** 2).sum()``\n",
    ">\n",
    ">$ v $ aka $ TSS $ ``((y_true - y_true.mean()) ** 2).sum()``\n",
    "\n",
    "Our RSS and TSS are now the same!\n",
    "\n",
    "If $ RSS = TSS $ then R-squared will always be 0 because any value divided by itself is 1. Therefore:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "R^2 &= 1 - \\frac{RSS}{TSS} \\\\\n",
    "\\\\\n",
    "&= 1 - \\frac{TSS}{TSS} \\\\\n",
    "\\\\\n",
    "&= 1 - 1 \\\\\n",
    "\\\\\n",
    "&= 0 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We would usually choose to use the model with the better score, and given that zero is greater than some negative number, we would choose to use the mean of *y* as our model. Even though zero is still rubbish, it is better than less than zero!\n",
    "\n",
    "### Proof that R-squared can be negative\n",
    "\n",
    "So far we have explained why not to use a model with R-squared less than zero but not yet shown how this can happen in the first place. Three possible scenarios which could lead to a negative R-squared score:\n",
    "\n",
    "1. Multiple features on the x axis\n",
    "1. The model has `fit_intercept=False`\n",
    "1. User error\n",
    "\n",
    "#### Multiple features on the x axis\n",
    "\n",
    "In our model above we had only one feature in `X` therefore we could caluclate the correlation $ R $ between `x` and `y`. If there are *multiple* features in `X` then there will also be *multiple* correlations, one from each `x` to `y`. It is **not** possible to calculate a combined $ R $, therefore when we have multiple features effectively *there is no $ R $*.\n",
    "\n",
    "This explains how it is possible to have a negative R-squared score even though it is the square of $ R $ since this is true **if and only if** we have just a single independent variable *x* in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9cbd0f",
   "metadata": {},
   "source": [
    "#### The model has `fit_intercept=False`\n",
    "\n",
    "The [scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) tells us that `LinearRegression()` can take the following parameters:\n",
    "\n",
    "> sklearn.linear_model.LinearRegression(\\*, *fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False*)\n",
    "\n",
    "Here we will look at `fit_intercept` which we can see has the default value `True` so the model will calculate the intercept value *c* for the line of best fit.\n",
    "\n",
    "If we explicitly set `fit_intercept=False` then the model will not calculate the intercept so *c* will always be zero. This is almost always a bad idea as we will demonstrate here, which is why the default is `fit_intercept=True`.\n",
    "\n",
    "There is a very small number of [complex special cases](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model) when you might need to force `c=0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data that has m=-1 and c=8 \n",
    "# i.e. a negative gradient and the intercept at 8\n",
    "df_fit_demo = df[['x','y']].copy()\n",
    "df_fit_demo['y'] = 8 - df_fit_demo['y']\n",
    "df_fit_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f987452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise what the line of best fit *should* look like\n",
    "g = sns.regplot(data=df_fit_demo, x='x', y='y')\n",
    "g.set(xlim=(0,5), ylim=(0,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d12144",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit_demo = df_fit_demo[['x']]\n",
    "y_fit_demo = df_fit_demo['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two models:\n",
    "# lr_t (t for true) with default, which is fit_intercept=True\n",
    "lr_t = LinearRegression().fit(X_fit_demo, y_fit_demo)\n",
    "\n",
    "# lr_f (f for false) with fit_intercept=False \n",
    "lr_f = LinearRegression(fit_intercept=False).fit(X_fit_demo, y_fit_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b565163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print equation for lr_t which has fit the intercept, data selected such that c=8\n",
    "m_t = lr_t.coef_[0]\n",
    "c_t = lr_t.intercept_\n",
    "print(f'Equation for line of best fit from lr_t model: y = {m_t}x + {c_t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print equations for lr_f which has not fit the intercept, so c=0\n",
    "m_f = lr_f.coef_[0]\n",
    "c_f = lr_f.intercept_\n",
    "print(f'Equation for line of best fit from lr_f model: y = {m_f}x + {c_f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35094914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot lines we need to create values for the x axis\n",
    "x = pd.DataFrame({'x': [0,1,2,3,4,5]})\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot LR equations using plot(x, y, label) where y = mx+c\n",
    "plt.plot(x, m_t*x + c_t, label='fit_intercept=True')\n",
    "plt.plot(x, m_f*x + c_f, label='fit_intercept=False')\n",
    "\n",
    "# plot the data \n",
    "sns.scatterplot(data=df_fit_demo, x='x', y='y')\n",
    "\n",
    "# draw the y axis at zero to show the intercepts clearly\n",
    "ax.spines['left'].set_position('zero')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f71b3",
   "metadata": {},
   "source": [
    "When the intercept is not fitted then it is left as zero. By definition the line will therefore always cross the x axis at zero.\n",
    "\n",
    "This can put the line in a really bad place which does not fit the data at all well, like we have here. If the fit is so bad that it is worse than simply using the mean of *y* then R-squared will be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bfcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the R-squared score of our lr_t model should be pretty good\n",
    "lr_t.score(X_fit_demo, y_fit_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84488a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the R-squared score of our lr_f model should be really, really bad\n",
    "lr_f.score(X_fit_demo, y_fit_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013685b",
   "metadata": {},
   "source": [
    "#### User error\n",
    "\n",
    "It can be easy to lose track of what everything is when working in a notebook like this. You might have mutliple models, you might also have multiple sources of data stored in different dataframes, you might also have extra layers of complexity such as train-test split or standardisation (neither of which we have covered here).\n",
    "\n",
    "It can also be easy to accidently overwrite a variable, such as `X`, with some other values which can lead to confusion.\n",
    "\n",
    "If we do not pass the correct data into the `.score(X,y)` function then we can get highly unexpected R-squared scores which might even be negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad774f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X,y_fit_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1005a36",
   "metadata": {},
   "source": [
    "What has happened here? The model named `lr` was made at the start of this notebook using the data from `X` so this is correct so far. However, the *y* values in `y_fit_demo` are from a completely different data source - nothing to do with the values in `X` and also nothing to do with the values of *y* that were used when we did `lr.fit(X,y)`.\n",
    "\n",
    "It is both possible and necessary sometimes to score a linear regression model with some other values of *y* that were not used to fit the model (which is the core principle of train-test split) but we need to make sure that what we do with `.score(X,y)` makes sense to avoid funky results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41647f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further reading\n",
    "\n",
    "Some of these have been included in the above notebook, others are additional sources:\n",
    "\n",
    "**Scikit learn**\n",
    "* [LinearRegression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on scikit learn\n",
    "\n",
    "**R-squared**\n",
    "* [How To Interpret R-squared in Regression Analysis](https://statisticsbyjim.com/regression/interpret-r-squared-regression/) on Statistics By Jim\n",
    "* [Why am I seeing a negative R^2 value?](https://help.desmos.com/hc/en-us/articles/202529139-Why-am-I-seeing-a-negative-R-2-value-) a nice, short explanation on desmos.com\n",
    "* [R-Squared](https://www.investopedia.com/terms/r/r-squared.asp) on investopedia.com - includes an excellent section on 'what is a good R-squared value?' in the context of investing\n",
    "* [Whatâ€™s a good value for R-squared?](https://people.duke.edu/~rnau/rsquared.htm) part of a comprehensive, academic series for anyone who wants to really understand linear regression analysis\n",
    "\n",
    "\n",
    "**The intercept (c)**\n",
    "\n",
    "* [When is it OK to remove the intercept in a linear regression model](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model) a slightly techical discussion on Cross Validated\n",
    "* [How to Interpret the Constant (Y Intercept) in Regression Analysis](https://statisticsbyjim.com/regression/interpret-constant-y-intercept-regression/) on Stats By Jim\n",
    "\n",
    "**Correlation (R)**\n",
    "\n",
    "* [Correlation coefficient](https://en.wikipedia.org/wiki/Correlation_coefficient) on wikipedia\n",
    "* [How to Interpret a Correlation Coefficient r](https://www.dummies.com/article/academics-the-arts/math/statistics/how-to-interpret-a-correlation-coefficient-r-169792/) by Statistics For Dummies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
